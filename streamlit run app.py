import streamlit as st
import pandas as pd
import io

# --- ‡§´‡§º‡§Ç‡§ï‡•ç‡§∂‡§®: ‡§°‡•á‡§ü‡§æ ‡§™‡•ç‡§∞‡•ã‡§∏‡•á‡§∏‡§ø‡§Ç‡§ó ---
def process_data(packed_file, rt_file, rto_file, seller_listings_file):
    """
    ‡§Ö‡§™‡§≤‡•ã‡§° ‡§ï‡•Ä ‡§ó‡§à ‡§´‡§º‡§æ‡§á‡§≤‡•ã‡§Ç ‡§ï‡•ã ‡§™‡§¢‡§º‡§§‡§æ ‡§π‡•à, SKU ID ‡§ï‡•á ‡§Ü‡§ß‡§æ‡§∞ ‡§™‡§∞ SKU Code ‡§î‡§∞ Seller SKU Code ‡§ï‡•ã ‡§Æ‡§∞‡•ç‡§ú ‡§ï‡§∞‡§§‡§æ ‡§π‡•à, 
    ‡§î‡§∞ ‡§™‡•ç‡§∞‡•ã‡§∏‡•á‡§∏ ‡§ï‡§ø‡§è ‡§ó‡§è ‡§°‡•á‡§ü‡§æ‡§´‡§º‡•ç‡§∞‡•á‡§Æ ‡§ï‡•ã ‡§µ‡§æ‡§™‡§∏ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§
    """
    
    # 1. Seller Listings File ‡§∏‡•á ‡§Æ‡•à‡§™‡§ø‡§Ç‡§ó ‡§°‡•á‡§ü‡§æ ‡§®‡§ø‡§ï‡§æ‡§≤‡•á‡§Ç
    try:
        # seller listings file ‡§ï‡•ã ‡§™‡§¢‡§º‡§ï‡§∞ 'sku id' ‡§î‡§∞ "seller sku code" ‡§ï‡•â‡§≤‡§Æ ‡§®‡§ø‡§ï‡§æ‡§≤‡•á‡§Ç
        # 'sku id' ‡§î‡§∞ "seller sku code" ‡§ï‡•â‡§≤‡§Æ ‡§ï‡•ã ‡§ï‡•ã‡§ü‡•á‡§∂‡§® ‡§Æ‡§æ‡§∞‡•ç‡§ï ‡§ï‡•á ‡§∏‡§æ‡§• ‡§™‡§¢‡§º‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è engine='python' ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç
        seller_df = pd.read_csv(seller_listings_file, engine='python')
        
        # ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§ï‡•â‡§≤‡§Æ ‡§ï‡•ã ‡§ö‡•Å‡§®‡•á‡§Ç ‡§î‡§∞ ‡§ï‡•â‡§≤‡§Æ ‡§ï‡•á ‡§®‡§æ‡§Æ ‡§∏‡•á ‡§Ö‡§§‡§ø‡§∞‡§ø‡§ï‡•ç‡§§ ‡§ï‡•ã‡§ü‡•á‡§∂‡§® ‡§Æ‡§æ‡§∞‡•ç‡§ï ‡§π‡§ü‡§æ‡§è‡§Ç
        # ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§¶‡•á‡§Ç: ‡§Ö‡§¨ ‡§π‡§Æ 'sku code' ‡§ï‡•á ‡§¨‡§ú‡§æ‡§Ø "seller sku code" ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç
        sku_map_df = seller_df[['sku id', 'sku code', 'seller sku code']].copy()
        sku_map_df.columns = sku_map_df.columns.str.strip().str.replace('"', '').str.replace(' ', '_')
        
        # ‡§ï‡•â‡§≤‡§Æ ‡§ï‡•á ‡§®‡§æ‡§Æ Normalize ‡§ï‡§∞‡•á‡§Ç
        sku_map_df.rename(columns={
            'sku_id': 'sku_id', 
            'sku_code': 'sku_code',
            'seller_sku_code': 'seller_sku_code'
        }, inplace=True)
        
        # ‡§°‡•Å‡§™‡•ç‡§≤‡§ø‡§ï‡•á‡§ü ‡§ï‡•ã ‡§π‡§ü‡§æ ‡§¶‡•á‡§Ç ‡§§‡§æ‡§ï‡§ø merging clean ‡§π‡•ã
        sku_map_df.drop_duplicates(subset=['sku_id'], inplace=True)
        
    except Exception as e:
        st.error(f"Seller Listings Report ‡§™‡§¢‡§º‡§®‡•á ‡§Æ‡•á‡§Ç ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø ‡§Ø‡§æ ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§ï‡•â‡§≤‡§Æ ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡•á: {e}")
        return None, None, None

    # ‡§°‡•á‡§ü‡§æ‡§´‡§º‡•ç‡§∞‡•á‡§Æ ‡§ï‡•Ä ‡§∏‡•Ç‡§ö‡•Ä ‡§¨‡§®‡§æ‡§è‡§Ç
    file_list = [
        ("Packed.csv", packed_file, 'packed_df'),
        ("RT..csv", rt_file, 'rt_df'),
        ("RTO.csv", rto_file, 'rto_df')
    ]
    
    processed_dfs = {}

    for file_name, uploaded_file, df_key in file_list:
        if uploaded_file is not None:
            st.info(f"Processing {file_name}...")
            try:
                # ‡§Ö‡§®‡•ç‡§Ø ‡§§‡•Ä‡§® ‡§´‡§º‡§æ‡§á‡§≤‡•ã‡§Ç ‡§ï‡•ã ‡§™‡§¢‡§º‡•á‡§Ç
                df = pd.read_csv(uploaded_file)
                
                # 'sku_id' ‡§ï‡•â‡§≤‡§Æ ‡§ï‡§æ ‡§®‡§æ‡§Æ Normalize ‡§ï‡§∞‡•á‡§Ç ‡§î‡§∞ ‡§Æ‡§∞‡•ç‡§ú ‡§ï‡•â‡§≤‡§Æ ‡§ï‡•ã ‡§™‡§π‡§ö‡§æ‡§®‡•á‡§Ç
                merge_column = None
                original_sku_id_name = None
                
                if 'sku_id' in df.columns:
                    merge_column = 'sku_id'
                    original_sku_id_name = 'sku_id'
                elif 'sku id' in df.columns:
                    original_sku_id_name = 'sku id'
                    df.rename(columns={'sku id': 'sku_id'}, inplace=True)
                    merge_column = 'sku_id'
                
                if merge_column is None:
                    st.warning(f"File **{file_name}** does not contain a suitable 'sku id' column. Skipping merge.")
                    processed_dfs[df_key] = df
                    continue

                # 'sku_id' ‡§ï‡•â‡§≤‡§Æ ‡§ï‡•ã string ‡§Æ‡•á‡§Ç ‡§¨‡§¶‡§≤‡•á‡§Ç ‡§§‡§æ‡§ï‡§ø merging ‡§†‡•Ä‡§ï ‡§∏‡•á ‡§π‡•ã
                df[merge_column] = df[merge_column].astype(str)
                sku_map_df['sku_id'] = sku_map_df['sku_id'].astype(str)
                
                # 'sku_id' ‡§ï‡•á ‡§Ü‡§ß‡§æ‡§∞ ‡§™‡§∞ 'seller_sku_code' ‡§î‡§∞ 'sku_code' ‡§ï‡•ã ‡§Æ‡§∞‡•ç‡§ú ‡§ï‡§∞‡•á‡§Ç
                merged_df = pd.merge(df, sku_map_df, on=merge_column, how='left')
                
                # seller_sku_code ‡§î‡§∞ sku_code ‡§ï‡•á ‡§Æ‡§ø‡§∏‡§ø‡§Ç‡§ó values ‡§ï‡•ã 'Not Found' ‡§∏‡•á ‡§≠‡§∞‡•á‡§Ç
                merged_df['seller_sku_code'] = merged_df['seller_sku_code'].fillna('Not Found')
                merged_df['sku_code'] = merged_df['sku_code'].fillna('Not Found')

                # 2. ‡§ï‡•â‡§≤‡§Æ ‡§ï‡•ã 'sku_id' ‡§ï‡•á ‡§Ü‡§ó‡•á Insert ‡§ï‡§∞‡•á‡§Ç
                # 'sku_id' ‡§ï‡•â‡§≤‡§Æ ‡§ï‡§æ Index ‡§™‡§§‡§æ ‡§ï‡§∞‡•á‡§Ç
                sku_id_index = merged_df.columns.get_loc('sku_id')
                
                # 'seller_sku_code' ‡§î‡§∞ 'sku_code' ‡§ï‡•ã ‡§π‡§ü‡§æ‡§®‡•á ‡§∏‡•á ‡§™‡§π‡§≤‡•á ‡§â‡§®‡§ï‡§æ ‡§°‡•á‡§ü‡§æ ‡§®‡§ø‡§ï‡§æ‡§≤ ‡§≤‡•á‡§Ç
                seller_sku_col = merged_df.pop('seller_sku_code')
                sku_code_col = merged_df.pop('sku_code')
                
                # 'seller_sku_code' ‡§ï‡•ã 'sku_id' ‡§ï‡•á ‡§†‡•Ä‡§ï ‡§Ü‡§ó‡•á Insert ‡§ï‡§∞‡•á‡§Ç (index + 1)
                merged_df.insert(sku_id_index + 1, 'seller_sku_code', seller_sku_col)
                
                # 'sku_code' ‡§ï‡•ã 'seller_sku_code' ‡§ï‡•á ‡§†‡•Ä‡§ï ‡§Ü‡§ó‡•á Insert ‡§ï‡§∞‡•á‡§Ç (index + 2)
                merged_df.insert(sku_id_index + 2, 'sku_code', sku_code_col)

                # ‡§Ø‡§¶‡§ø ‡§Æ‡•Ç‡§≤ 'sku id' ‡§ï‡•â‡§≤‡§Æ ‡§ï‡§æ ‡§®‡§æ‡§Æ 'sku id' ‡§•‡§æ, ‡§§‡•ã ‡§â‡§∏‡•á ‡§µ‡§æ‡§™‡§∏ ‡§†‡•Ä‡§ï ‡§ï‡§∞‡•á‡§Ç (‡§Ø‡§π optional ‡§π‡•à, ‡§≤‡•á‡§ï‡§ø‡§® ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§Ö‡§≠‡•ç‡§Ø‡§æ‡§∏ ‡§π‡•à)
                if original_sku_id_name == 'sku id':
                    merged_df.rename(columns={'sku_id': 'sku id'}, inplace=True)
                
                processed_dfs[df_key] = merged_df
                st.success(f"**{file_name}** successfully processed. 'seller_sku_code' and 'sku_code' added next to 'sku id'.")

            except Exception as e:
                st.error(f"Error reading or processing **{file_name}**: {e}")
                processed_dfs[df_key] = None
        else:
            processed_dfs[df_key] = None
    
    return processed_dfs.get('packed_df'), processed_dfs.get('rt_df'), processed_dfs.get('rto_df')

# --- ‡§´‡§º‡§Ç‡§ï‡•ç‡§∂‡§®: CSV ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§°‡§∞ ---
def convert_df_to_csv(df):
    """
    Pandas DataFrame ‡§ï‡•ã CSV string ‡§Æ‡•á‡§Ç ‡§¨‡§¶‡§≤‡§§‡§æ ‡§π‡•à‡•§
    """
    return df.to_csv(index=False).encode('utf-8')

# --- Streamlit ‡§°‡•à‡§∂‡§¨‡•ã‡§∞‡•ç‡§° ‡§≤‡•á‡§Ü‡§â‡§ü ---
def main():
    st.set_page_config(
        page_title="SKU Code Merger Dashboard",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("üõçÔ∏è SKU Code Merger Dashboard (Updated)")
    st.markdown("---")
    
    st.sidebar.header("üìÅ Upload Your Files")
    
    # ‡§´‡§º‡§æ‡§á‡§≤ ‡§Ö‡§™‡§≤‡•ã‡§°‡§∞‡•ç‡§∏
    seller_listings_file = st.sidebar.file_uploader(
        "Upload **Seller Listings Report.csv** (Required)", 
        type=['csv'],
        key="seller"
    )
    packed_file = st.sidebar.file_uploader(
        "Upload **Packed.csv**", 
        type=['csv'],
        key="packed"
    )
    rt_file = st.sidebar.file_uploader(
        "Upload **RT..csv**", 
        type=['csv'],
        key="rt"
    )
    rto_file = st.sidebar.file_uploader(
        "Upload **RTO.csv**", 
        type=['csv'],
        key="rto"
    )
    
    st.markdown("---")
    
    if st.sidebar.button("üöÄ Start Processing & Merge"):
        if seller_listings_file is None:
            st.error("Please upload the **Seller Listings Report.csv** to start the process.")
        else:
            with st.spinner("Merging Seller SKU and SKU Codes... Please wait."):
                # ‡§°‡•á‡§ü‡§æ ‡§™‡•ç‡§∞‡•ã‡§∏‡•á‡§∏‡§ø‡§Ç‡§ó
                packed_df_merged, rt_df_merged, rto_df_merged = process_data(
                    packed_file, rt_file, rto_file, seller_listings_file
                )

            st.header("‚úÖ Processing Complete")
            
            # --- ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ ‡§î‡§∞ ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§° ‡§∏‡•á‡§ï‡•ç‡§∂‡§® ---
            
            # Packed Dataframe
            if packed_df_merged is not None:
                st.subheader("1. Packed Data (Merged)")
                st.dataframe(packed_df_merged.head())
                csv_packed = convert_df_to_csv(packed_df_merged)
                st.download_button(
                    label="Download Packed_Merged.csv",
                    data=csv_packed,
                    file_name='Packed_Merged.csv',
                    mime='text/csv',
                )
                st.markdown("---")
                
            # RT Dataframe
            if rt_df_merged is not None:
                st.subheader("2. RT Data (Merged)")
                st.dataframe(rt_df_merged.head())
                csv_rt = convert_df_to_csv(rt_df_merged)
                st.download_button(
                    label="Download RT_Merged.csv",
                    data=csv_rt,
                    file_name='RT_Merged.csv',
                    mime='text/csv',
                )
                st.markdown("---")
                
            # RTO Dataframe
            if rto_df_merged is not None:
                st.subheader("3. RTO Data (Merged)")
                st.dataframe(rto_df_merged.head())
                csv_rto = convert_df_to_csv(rto_df_merged)
                st.download_button(
                    label="Download RTO_Merged.csv",
                    data=csv_rto,
                    file_name='RTO_Merged.csv',
                    mime='text/csv',
                )
                st.markdown("---")
                
            if packed_df_merged is None and rt_df_merged is None and rto_df_merged is None:
                st.warning("No other data files were successfully uploaded or processed.")

# Streamlit App ‡§ï‡•ã ‡§∞‡§® ‡§ï‡§∞‡•á‡§Ç
if __name__ == "__main__":
    main()

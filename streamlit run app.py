import streamlit as st
import pandas as pd
import io
import zipfile

# --- ZIP ‡§π‡•à‡§Ç‡§°‡§≤‡§ø‡§Ç‡§ó ‡§´‡§º‡§Ç‡§ï‡•ç‡§∂‡§®‡•ç‡§∏ (‡§®‡•ã ‡§ö‡•á‡§Ç‡§ú) ---

def handle_packed_rto_zip_upload(zip_file):
    """Packed/RT/RTO ZIP ‡§´‡§º‡§æ‡§á‡§≤ ‡§ï‡•ã ‡§è‡§ï‡•ç‡§∏‡§ü‡•ç‡§∞‡•à‡§ï‡•ç‡§ü ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§"""
    if zip_file is None:
        return None, None, None, False
    csv_data = {}
    required_files = ["Packed.csv", "RT..csv", "RTO.csv"]
    st.info("Extracting files from the Data ZIP archive...")
    try:
        with zipfile.ZipFile(zip_file, 'r') as z:
            for file_name in required_files:
                try:
                    file_content = z.read(file_name).decode('utf-8', errors='ignore')
                    csv_data[file_name] = io.StringIO(file_content)
                except KeyError:
                    st.error(f"Required file **{file_name}** not found in the Data ZIP archive.")
                    return None, None, None, False
        return csv_data.get("Packed.csv"), csv_data.get("RT..csv"), csv_data.get("RTO.csv"), True
    except Exception as e:
        st.error(f"An error occurred during Data ZIP file extraction: {e}")
        return None, None, None, False

def handle_settlement_zip(zip_file, process_name):
    """Settlement ZIP ‡§´‡§º‡§æ‡§á‡§≤ ‡§ï‡•ã ‡§è‡§ï‡•ç‡§∏‡§ü‡•ç‡§∞‡•à‡§ï‡•ç‡§ü ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‡§î‡§∞ ‡§∏‡§≠‡•Ä CSV ‡§´‡§º‡§æ‡§á‡§≤‡•ã‡§Ç ‡§ï‡•ã list of StringIO objects ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§µ‡§æ‡§™‡§∏ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§"""
    if zip_file is None:
        return []
    extracted_csv_objects = []
    st.info(f"Extracting files from the {process_name} ZIP archive...")
    try:
        with zipfile.ZipFile(zip_file, 'r') as z:
            for file_name in z.namelist():
                if file_name.lower().endswith('.csv') and not file_name.startswith('__'):
                    st.write(f"Found CSV: {file_name}")
                    file_content = z.read(file_name).decode('utf-8', errors='ignore')
                    extracted_csv_objects.append(io.StringIO(file_content))
            if not extracted_csv_objects:
                st.warning(f"No CSV files found inside the {process_name} ZIP.")
                return []
            return extracted_csv_objects
    except Exception as e:
        st.error(f"An error occurred during {process_name} ZIP file extraction: {e}")
        return []

# --- ‡§´‡§º‡§Ç‡§ï‡•ç‡§∂‡§®: Outstanding CSV ‡§π‡•à‡§Ç‡§°‡§≤‡§ø‡§Ç‡§ó (‡§®‡§Ø‡§æ) ---
def handle_outstanding_csv(csv_file):
    """Outstanding CSV ‡§´‡§º‡§æ‡§á‡§≤ ‡§ï‡•ã StringIO ‡§ë‡§¨‡•ç‡§ú‡•á‡§ï‡•ç‡§ü ‡§Æ‡•á‡§Ç ‡§¨‡§¶‡§≤‡§§‡§æ ‡§π‡•à‡•§"""
    if csv_file is None:
        return []
    try:
        # ‡§´‡§º‡§æ‡§á‡§≤ ‡§ï‡•ã ‡§∏‡•Ä‡§ß‡•á StringIO ‡§ë‡§¨‡•ç‡§ú‡•á‡§ï‡•ç‡§ü ‡§Æ‡•á‡§Ç ‡§¨‡§¶‡§≤‡•á‡§Ç
        file_content = csv_file.getvalue().decode('utf-8', errors='ignore')
        return [io.StringIO(file_content)]
    except Exception as e:
        st.error(f"An error occurred during Outstanding CSV file handling: {e}")
        return []

# --- SKU Merger (‡§®‡•ã ‡§ö‡•á‡§Ç‡§ú) ---

def process_sku_merger(packed_file_obj, rt_file_obj, rto_file_obj, seller_listings_file):
    # ... (Logic remains the same as previous response) ...
    st.subheader("1. SKU Code Merger Process")
    
    try:
        seller_df = pd.read_csv(seller_listings_file, engine='python')
        sku_map_df = seller_df[['sku id', 'sku code', 'seller sku code']].copy()
        sku_map_df.columns = sku_map_df.columns.str.strip().str.replace('"', '').str.replace(' ', '_')
        sku_map_df.rename(columns={'sku_id': 'sku_id', 'sku_code': 'sku_code', 'seller_sku_code': 'seller_sku_code'}, inplace=True)
        sku_map_df.drop_duplicates(subset=['sku_id'], inplace=True)
        sku_map_df['sku_id'] = sku_map_df['sku_id'].astype(str)
    except Exception as e:
        st.error(f"Seller Listings Report ‡§™‡§¢‡§º‡§®‡•á ‡§Æ‡•á‡§Ç ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø ‡§Ø‡§æ ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§ï‡•â‡§≤‡§Æ ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡•á: {e}")
        return None, None, None

    file_list = [
        ("Packed.csv", packed_file_obj, 'packed_df'),
        ("RT..csv", rt_file_obj, 'rt_df'),
        ("RTO.csv", rto_file_obj, 'rto_df')
    ]
    
    processed_dfs = {}

    for file_name, file_obj, df_key in file_list:
        if file_obj is not None:
            try:
                df = pd.read_csv(file_obj)
                
                merge_column = None
                original_sku_id_name = None
                
                if 'sku_id' in df.columns:
                    merge_column = 'sku_id'
                    original_sku_id_name = 'sku_id'
                elif 'sku id' in df.columns:
                    original_sku_id_name = 'sku id'
                    df.rename(columns={'sku id': 'sku_id'}, inplace=True)
                    merge_column = 'sku_id'
                
                if merge_column is None:
                    st.warning(f"File **{file_name}** does not contain a suitable 'sku id' column. Data not merged.")
                    processed_dfs[df_key] = df
                    continue

                df[merge_column] = df[merge_column].astype(str)
                merged_df = pd.merge(df, sku_map_df, on=merge_column, how='left')
                
                merged_df['seller_sku_code'] = merged_df['seller_sku_code'].fillna('Not Found')
                merged_df['sku_code'] = merged_df['sku_code'].fillna('Not Found')

                sku_id_index = merged_df.columns.get_loc('sku_id')
                
                seller_sku_col = merged_df.pop('seller_sku_code')
                sku_code_col = merged_df.pop('sku_code')
                
                merged_df.insert(sku_id_index + 1, 'seller_sku_code', seller_sku_col)
                merged_df.insert(sku_id_index + 2, 'sku_code', sku_code_col)

                if original_sku_id_name == 'sku id':
                    merged_df.rename(columns={'sku_id': 'sku id'}, inplace=True)
                
                processed_dfs[df_key] = merged_df
                st.success(f"**{file_name}** successfully processed.")

            except Exception as e:
                st.error(f"Error reading or processing **{file_name}**: {e}")
                processed_dfs[df_key] = None
        else:
            processed_dfs[df_key] = None
    
    return processed_dfs.get('packed_df'), processed_dfs.get('rt_df'), processed_dfs.get('rto_df')

# --- ‡§´‡§º‡§Ç‡§ï‡•ç‡§∂‡§®: ‡§ï‡§Ç‡§¨‡§æ‡§á‡§Ç‡§° ‡§∏‡•á‡§ü‡§≤‡§Æ‡•á‡§Ç‡§ü Pivot Processor (‡§Ö‡§™‡§°‡•á‡§ü‡•á‡§°) ---

def process_combined_settlement(all_csv_objects):
    """
    ‡§∏‡§≠‡•Ä Prepaid, Postpaid, ‡§î‡§∞ Outstanding data ‡§ï‡•ã ‡§™‡§¢‡§º‡§§‡§æ ‡§π‡•à ‡§î‡§∞ ‡§è‡§ï Final Merged Pivot Table ‡§¨‡§®‡§æ‡§§‡§æ ‡§π‡•à‡•§
    """
    st.subheader("2. Combined Settlement & Outstanding Pivot")
    
    if not all_csv_objects:
        st.warning("No settlement or outstanding files were uploaded or extracted successfully.")
        return None

    all_dfs = []
    
    # ‡§Ö‡§™‡•á‡§ï‡•ç‡§∑‡§ø‡§§ ‡§ï‡•â‡§≤‡§Æ ‡§®‡§æ‡§Æ (Normalization ‡§ï‡•á ‡§≤‡§ø‡§è)
    TARGET_COL_ID = 'order_release_id'
    
    # ‡§¶‡•ã ‡§∏‡§Ç‡§≠‡§æ‡§µ‡§ø‡§§ ‡§∞‡§æ‡§∂‡§ø ‡§ï‡•â‡§≤‡§Æ - Settled_Amount (Prepaid/Postpaid) ‡§î‡§∞ Unsettled_Amount (Outstanding)
    TARGET_COL_AMOUNT_SETTLED = 'Settled_Amount'
    TARGET_COL_AMOUNT_UNSETTLED = 'Unsettled_Amount'
    
    # ‡§â‡§® ‡§®‡§æ‡§Æ‡•ã‡§Ç ‡§ï‡•ã ‡§ú‡§ø‡§®‡•ç‡§π‡•á‡§Ç ‡§π‡§Æ‡•á‡§Ç ‡§Æ‡•à‡§ö ‡§ï‡§∞‡§®‡§æ ‡§π‡•à (lowercase, underscores removed)
    MATCH_ID = TARGET_COL_ID.lower().replace('_', '')
    MATCH_SETTLED = TARGET_COL_AMOUNT_SETTLED.lower().replace('_', '')
    MATCH_UNSETTLED = TARGET_COL_AMOUNT_UNSETTLED.lower().replace('_', '')


    for i, file_obj in enumerate(all_csv_objects):
        file_name = f"Combined_Payment_File_{i+1}"
        try:
            df = pd.read_csv(file_obj)
            
            # ‡§ï‡•â‡§≤‡§Æ ‡§®‡§æ‡§Æ‡•ã‡§Ç ‡§ï‡•ã Normalize ‡§ï‡§∞‡•á‡§Ç: Lowercase + Spaces/Quotes ‡§π‡§ü‡§æ‡§è‡§Å
            normalized_cols = {col: col.strip().replace('"', '').lower().replace('_', '') for col in df.columns}
            
            found_id_name = None
            found_amount_name = None
            amount_type = None # Settled ‡§Ø‡§æ Unsettled

            for original_name, norm_name in normalized_cols.items():
                if norm_name == MATCH_ID:
                    found_id_name = original_name
                # Settled_Amount ‡§ï‡•ã ‡§™‡§π‡§≤‡•á ‡§ñ‡•ã‡§ú‡•á‡§Ç
                if norm_name == MATCH_SETTLED:
                    found_amount_name = original_name
                    amount_type = 'Settled'
                # ‡§Ö‡§ó‡§∞ Settled_Amount ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ, ‡§§‡•ã Unsettled_Amount ‡§ñ‡•ã‡§ú‡•á‡§Ç
                elif norm_name == MATCH_UNSETTLED:
                    found_amount_name = original_name
                    amount_type = 'Unsettled'
            
            if not found_id_name or not found_amount_name:
                st.error(f"File **{file_name}** is missing required ID or Amount columns. Skipping.")
                continue

            # ‡§ï‡•á‡§µ‡§≤ ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï ‡§ï‡•â‡§≤‡§Æ ‡§ö‡•Å‡§®‡•á‡§Ç
            df_subset = df[[found_id_name, found_amount_name]].copy()
            
            # ‡§∞‡§æ‡§∂‡§ø ‡§ï‡•â‡§≤‡§Æ ‡§ï‡•ã ‡§è‡§ï ‡§∏‡•ç‡§ü‡•à‡§Ç‡§°‡§∞‡•ç‡§° ‡§®‡§æ‡§Æ ‡§¶‡•á‡§Ç (Total_Amount)
            df_subset.rename(columns={
                found_id_name: TARGET_COL_ID, 
                found_amount_name: 'Total_Amount'
            }, inplace=True)
            
            # Total_Amount ‡§ï‡•ã numeric ‡§Æ‡•á‡§Ç ‡§¨‡§¶‡§≤‡•á‡§Ç 
            df_subset['Total_Amount'] = pd.to_numeric(df_subset['Total_Amount'], errors='coerce')
            
            all_dfs.append(df_subset)
            st.success(f"**{file_name}** read successfully. Amount found: **{amount_type}**.")
            
        except Exception as e:
            st.error(f"Error reading **{file_name}**: {e}")
            
    if not all_dfs:
        st.error("No combined payment data could be processed successfully.")
        return None
        
    combined_df = pd.concat(all_dfs, ignore_index=True)
    
    # Final Pivot Table ‡§¨‡§®‡§æ‡§è‡§Å (‡§∏‡§≠‡•Ä ‡§°‡•á‡§ü‡§æ ‡§ï‡•ã ‡§Æ‡§ø‡§≤‡§æ‡§ï‡§∞)
    pivot_table = combined_df.groupby(TARGET_COL_ID)['Total_Amount'].sum().reset_index()
    pivot_table.rename(columns={'Total_Amount': 'Total_Settled_Outstanding_Amount'}, inplace=True)
    
    st.success("Final Merged Payment & Outstanding Pivot Table created successfully.")
    return pivot_table


# --- ‡§´‡§º‡§Ç‡§ï‡•ç‡§∂‡§®: ‡§Æ‡§≤‡•ç‡§ü‡•Ä-‡§∂‡•Ä‡§ü Excel ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§°‡§∞ (‡§®‡•ã ‡§ö‡•á‡§Ç‡§ú) ---

def convert_dfs_to_excel(df_packed, df_rt, df_rto, df_merged_pivot):
    """
    ‡§ö‡§æ‡§∞ DataFrames ‡§ï‡•ã ‡§è‡§ï Excel ‡§´‡§º‡§æ‡§á‡§≤ ‡§ï‡•Ä ‡§Ö‡§≤‡§ó-‡§Ö‡§≤‡§ó ‡§∂‡•Ä‡§ü‡•ç‡§∏ ‡§Æ‡•á‡§Ç ‡§≤‡§ø‡§ñ‡§§‡§æ ‡§π‡•à (Sheet 4 ‡§™‡§∞ Merged Pivot Table)‡•§
    """
    output = io.BytesIO()
    
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        if df_packed is not None:
            df_packed.to_excel(writer, sheet_name='Packed', index=False) # Sheet 1
        if df_rt is not None:
            df_rt.to_excel(writer, sheet_name='RT', index=False)         # Sheet 2
        if df_rto is not None:
            df_rto.to_excel(writer, sheet_name='RTO', index=False)       # Sheet 3
        if df_merged_pivot is not None:
            df_merged_pivot.to_excel(writer, sheet_name='Merged_Payment_Pivot', index=False) # Sheet 4 (Final Pivot)
    
    processed_excel_data = output.getvalue()
    return processed_excel_data


# --- Streamlit ‡§°‡•à‡§∂‡§¨‡•ã‡§∞‡•ç‡§° ‡§≤‡•á‡§Ü‡§â‡§ü (‡§Ö‡§™‡§°‡•á‡§ü‡•á‡§°) ---

def main():
    st.set_page_config(
        page_title="SKU & Settlement Data Processor",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("üõçÔ∏è SKU & Settlement Data Processor")
    st.markdown("---")
    
    # ----------------------------------------------------
    #                  SIDEBAR UPLOADERS
    # ----------------------------------------------------
    st.sidebar.header("üìÅ 1. Files for SKU Merger")
    
    seller_listings_file = st.sidebar.file_uploader(
        "Upload **Seller Listings Report.csv** (Required)", 
        type=['csv'],
        key="seller"
    )
    data_zip_file = st.sidebar.file_uploader(
        "Upload **Packed, RT, RTO files as a ZIP**", 
        type=['zip'],
        key="data_zip"
    )
    
    st.sidebar.markdown("---")
    st.sidebar.header("üßæ 2. Payment Files (Settled & Outstanding)")
    
    prepaid_zip_file = st.sidebar.file_uploader(
        "Upload **Prepaid Settlement CSVs as a single ZIP**", 
        type=['zip'],
        key="prepaid_zip"
    )
    
    postpaid_zip_file = st.sidebar.file_uploader(
        "Upload **Postpaid Settlement CSVs as a single ZIP**", 
        type=['zip'],
        key="postpaid_zip"
    )
    
    # ‡§®‡§Ø‡§æ CSV ‡§Ö‡§™‡§≤‡•ã‡§°‡§∞
    outstanding_csv_file = st.sidebar.file_uploader(
        "Upload **Outstanding Payment CSV**", 
        type=['csv'],
        key="outstanding_csv"
    )
    
    st.markdown("---")
    
    df_merged_pivot = None 
    packed_df_merged, rt_df_merged, rto_df_merged = None, None, None
    
    if st.sidebar.button("üöÄ Start All Processing"):
        
        # ----------------------------------------------------
        #             Combined Payment Execution
        # ----------------------------------------------------
        
        st.header("--- Combined Payment & Outstanding Pivot Results ---")
        
        # ‡§∏‡§≠‡•Ä sources ‡§∏‡•á CSV objects ‡§á‡§ï‡§ü‡•ç‡§†‡§æ ‡§ï‡§∞‡•á‡§Ç
        prepaid_objects = handle_settlement_zip(prepaid_zip_file, "Prepaid")
        postpaid_objects = handle_settlement_zip(postpaid_zip_file, "Postpaid")
        outstanding_objects = handle_outstanding_csv(outstanding_csv_file) # ‡§®‡§Ø‡§æ
        
        all_csv_objects = prepaid_objects + postpaid_objects + outstanding_objects
        
        if all_csv_objects:
            with st.spinner("Processing all payment files and creating Merged Pivot Table..."):
                df_merged_pivot = process_combined_settlement(all_csv_objects)
        else:
            st.warning("Skipping Combined Pivot: No payment files were uploaded successfully.")

        
        # ----------------------------------------------------
        #                  SKU Merger Execution
        # ----------------------------------------------------
        st.header("--- SKU Code Merger Results ---")
        if seller_listings_file is None or data_zip_file is None:
            st.warning("Skipping SKU Merger: Required files not uploaded.")
            packed_df_merged, rt_df_merged, rto_df_merged = None, None, None
        else:
            packed_obj, rt_obj, rto_obj, success = handle_packed_rto_zip_upload(data_zip_file)
            
            if success:
                with st.spinner("Merging SKU data..."):
                    packed_df_merged, rt_df_merged, rto_df_merged = process_sku_merger(
                        packed_obj, rt_obj, rto_obj, seller_listings_file
                    )
            
        
        # ----------------------------------------------------
        #             Final Excel Generation
        # ----------------------------------------------------
        st.header("--- üíæ Final Excel Download ---")
        
        if packed_df_merged is not None or rt_df_merged is not None or rto_df_merged is not None or df_merged_pivot is not None:
            with st.spinner("Generating Multi-Sheet Excel Workbook (Packed, RT, RTO, Merged_Payment_Pivot)..."):
                excel_data = convert_dfs_to_excel(packed_df_merged, rt_df_merged, rto_df_merged, df_merged_pivot)
            
            st.success("‚úÖ Multi-sheet Excel file is ready. It contains: Packed (Sheet 1), RT (Sheet 2), RTO (Sheet 3), and Merged_Payment_Pivot (Sheet 4).")
            
            st.download_button(
                label="‚¨áÔ∏è Download Complete Merged Data (Excel)",
                data=excel_data,
                file_name='Merged_SKU_Settlement_Outstanding_Report_Final.xlsx',
                mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
                key='download_excel'
            )
            st.markdown("---")
            
            st.subheader("Preview of Merged Payment & Outstanding Pivot (Sheet 4)")
            if df_merged_pivot is not None:
                 st.dataframe(df_merged_pivot.head(10))
            else:
                st.info("Merged Payment & Outstanding Pivot data was not generated.")

        else:
            st.error("No data files could be processed successfully to generate the final Excel report.")


# Streamlit App ‡§ï‡•ã ‡§∞‡§® ‡§ï‡§∞‡•á‡§Ç
if __name__ == "__main__":
    main()
